# Docker Compose version
version: '3.8'

# Define all the services (containers) that make up our application
services:

  # The Backend Service (FastAPI)
  backend:
    # Build the Docker image from the Dockerfile in the './backend' directory
    build: ./backend
    # Map port 8000 on the host machine to port 8000 in the container
    # This allows us to access the API at http://localhost:8000
    ports: 
      - "8000:8000"
    # Mount the local './backend/app' directory into the container at '/code/app'
    # This enables hot-reloading: changes in your local code will reflect in the container instantly
    volumes:
      - ./backend/app:/code/app
    
    env_file:
      - .env
    # ส่ง Token ที่อ่านได้เข้าไปใน Container
    environment:
      - MODEL_PATH=Pathfinder9362/Student-Mind-Mate-AI-v2
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_TOKEN}
      - VLLM_LOGGING_LEVEL=DEBUG
    


  # The Frontend Service (React)
  frontend:
    # Build the Docker image from the Dockerfile in the './frontend' directory
    build: ./frontend
    # Map port 5173 on the host machine to port 80 in the container
    # The React dev server runs on 5173, and the Nginx server inside the container serves on port 80
    ports:
      - "5173:80"
    # Mount the local './frontend/src' directory for hot-reloading
    volumes:
      - ./frontend/src:/app/src
    # This service should only start after the 'backend' service is ready
    depends_on:
      - backend

# Note: In a production setup with vLLM, you would add another service here:
  vllm_server:
    image: vllm/vllm-openai:latest
    environment:
      - VLLM_LOGGING_LEVEL=DEBUG
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  # ... (GPU configurations, model loading commands) ...
  # And the 'backend' would have depends_on: - vllm_server